{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Imports </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "import json\n",
    "import traceback\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from info_nce import InfoNCE\n",
    "from tqdm import tqdm\n",
    "from collections import OrderedDict\n",
    "import tqdm as notebook_tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import make_grid\n",
    "from PIL import Image\n",
    "import kornia\n",
    "from kornia.augmentation.container import AugmentationSequential\n",
    "from pytorch_msssim import ssim\n",
    "\n",
    "import utils\n",
    "from models import Voxel2StableDiffusionModel\n",
    "from convnext import ConvnextXL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_ddp():\n",
    "    import torch.distributed as dist\n",
    "    env_dict = {\n",
    "        key: os.environ.get(key)\n",
    "        for key in (\"MASTER_ADDR\", \"MASTER_PORT\", \"RANK\",\n",
    "                    \"LOCAL_RANK\", \"WORLD_SIZE\", \"NUM_GPUS\")\n",
    "    }\n",
    "    rank = int(os.environ[\"RANK\"])\n",
    "    local_rank = int(os.environ[\"LOCAL_RANK\"])\n",
    "    n = int(os.environ.get(\"NUM_GPUS\", 8))\n",
    "    device_ids = list(\n",
    "        range(local_rank * n, (local_rank + 1) * n)\n",
    "    )\n",
    "\n",
    "    if local_rank == 0:\n",
    "        print(f\"[{os.getpid()}] Initializing process group with: {env_dict}\")\n",
    "    dist.init_process_group(backend=\"nccl\")\n",
    "    print(\n",
    "        f\"[{os.getpid()}] world_size = {dist.get_world_size()}, \"\n",
    "        + f\"rank = {dist.get_rank()}, backend={dist.get_backend()}\"\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"[{os.getpid()}] rank = {dist.get_rank()} ({rank}), \"\n",
    "        + f\"world_size = {dist.get_world_size()}, n = {n}, device_ids = {device_ids}\"\n",
    "    )\n",
    "    device = torch.device(\"cuda\", local_rank)\n",
    "    return local_rank, device, n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_rank = 0\n",
    "device = torch.device(\"cuda:0\")\n",
    "num_devices = 1\n",
    "distributed = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AutoencoderKL(\n",
       "  (encoder): Encoder(\n",
       "    (conv_in): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (down_blocks): ModuleList(\n",
       "      (0): DownEncoderBlock2D(\n",
       "        (resnets): ModuleList(\n",
       "          (0): ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "          )\n",
       "          (1): ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "          )\n",
       "        )\n",
       "        (downsamplers): ModuleList(\n",
       "          (0): Downsample2D(\n",
       "            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): DownEncoderBlock2D(\n",
       "        (resnets): ModuleList(\n",
       "          (0): ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "            (conv_shortcut): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "          )\n",
       "        )\n",
       "        (downsamplers): ModuleList(\n",
       "          (0): Downsample2D(\n",
       "            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): DownEncoderBlock2D(\n",
       "        (resnets): ModuleList(\n",
       "          (0): ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "            (conv_shortcut): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "          )\n",
       "        )\n",
       "        (downsamplers): ModuleList(\n",
       "          (0): Downsample2D(\n",
       "            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): DownEncoderBlock2D(\n",
       "        (resnets): ModuleList(\n",
       "          (0): ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "          )\n",
       "          (1): ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (mid_block): UNetMidBlock2D(\n",
       "      (attentions): ModuleList(\n",
       "        (0): AttentionBlock(\n",
       "          (group_norm): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (proj_attn): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (resnets): ModuleList(\n",
       "        (0): ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "        )\n",
       "        (1): ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (conv_norm_out): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "    (conv_act): SiLU()\n",
       "    (conv_out): Conv2d(512, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (conv_in): Conv2d(4, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (up_blocks): ModuleList(\n",
       "      (0): UpDecoderBlock2D(\n",
       "        (resnets): ModuleList(\n",
       "          (0): ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "          )\n",
       "          (1): ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "          )\n",
       "          (2): ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "          )\n",
       "        )\n",
       "        (upsamplers): ModuleList(\n",
       "          (0): Upsample2D(\n",
       "            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): UpDecoderBlock2D(\n",
       "        (resnets): ModuleList(\n",
       "          (0): ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "          )\n",
       "          (1): ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "          )\n",
       "          (2): ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "          )\n",
       "        )\n",
       "        (upsamplers): ModuleList(\n",
       "          (0): Upsample2D(\n",
       "            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): UpDecoderBlock2D(\n",
       "        (resnets): ModuleList(\n",
       "          (0): ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "            (conv_shortcut): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "          )\n",
       "          (2): ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "          )\n",
       "        )\n",
       "        (upsamplers): ModuleList(\n",
       "          (0): Upsample2D(\n",
       "            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): UpDecoderBlock2D(\n",
       "        (resnets): ModuleList(\n",
       "          (0): ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "            (conv_shortcut): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "          )\n",
       "          (2): ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (mid_block): UNetMidBlock2D(\n",
       "      (attentions): ModuleList(\n",
       "        (0): AttentionBlock(\n",
       "          (group_norm): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (proj_attn): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (resnets): ModuleList(\n",
       "        (0): ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "        )\n",
       "        (1): ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (conv_norm_out): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "    (conv_act): SiLU()\n",
       "    (conv_out): Conv2d(128, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       "  (quant_conv): Conv2d(8, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (post_quant_conv): Conv2d(4, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from diffusers.models import AutoencoderKL\n",
    "autoenc = AutoencoderKL(\n",
    "    down_block_types=['DownEncoderBlock2D', 'DownEncoderBlock2D', 'DownEncoderBlock2D', 'DownEncoderBlock2D'],\n",
    "    up_block_types=['UpDecoderBlock2D', 'UpDecoderBlock2D', 'UpDecoderBlock2D', 'UpDecoderBlock2D'],\n",
    "    block_out_channels=[128, 256, 512, 512],\n",
    "    layers_per_block=2,\n",
    "    sample_size=256\n",
    ")\n",
    "autoenc.load_state_dict(torch.load('../train_logs/models/sd_image_var_autoenc.pth'))\n",
    "autoenc.requires_grad_(False)\n",
    "autoenc.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Configurations </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"autoencoder\"\n",
    "modality = \"image\" # (\"image\", \"text\")\n",
    "image_var = 'images' if modality=='image' else None  # trial\n",
    "clamp_embs = False # clamp embeddings to (-1.5, 1.5)\n",
    "\n",
    "voxel_dims = 1 # 1 for flattened;  3 for 3d\n",
    "n_samples_save = 4 # how many SD samples from train and val to save\n",
    "\n",
    "use_reconst = False\n",
    "batch_size = 8\n",
    "num_epochs = 120\n",
    "lr_scheduler = 'cycle'\n",
    "initial_lr = 1e-3\n",
    "max_lr = 5e-4\n",
    "first_batch = False\n",
    "ckpt_saving = True\n",
    "ckpt_interval = 24\n",
    "save_at_end = False\n",
    "use_mp = False\n",
    "remote_data = False\n",
    "data_commit = \"avg\"  # '9947586218b6b7c8cab804009ddca5045249a38d'\n",
    "mixup_pct = -1\n",
    "use_cont = True\n",
    "use_sobel_loss = False\n",
    "use_blurred_training = False\n",
    "\n",
    "use_full_trainset = True\n",
    "subj_id = \"01\"\n",
    "seed = 0\n",
    "# ckpt_path = \"../train_logs/models/autoencoder_final/test/ckpt-epoch015.pth\"\n",
    "ckpt_path1 = \"../train_logs/autoencoder_subj01_4x_locont_no_reconst/epoch120.pth\"\n",
    "ckpt_path = None\n",
    "cont_model = 'cnx'\n",
    "resume_from_ckpt = False\n",
    "ups_mode = '4x'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: not using cudnn.deterministic\n"
     ]
    }
   ],
   "source": [
    "# need non-deterministic CuDNN for conv3D to work\n",
    "utils.seed_everything(seed+local_rank, cudnn_deterministic=False)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "# if running command line, read in args or config file values and override above params\n",
    "try:\n",
    "    config_keys = [k for k,v in globals().items() if not k.startswith('_') \\\n",
    "                   and isinstance(v, (int, float, bool, str))]\n",
    "    exec(open('configurator.py').read()) # overrides from command line or config file\n",
    "    config = {k: globals()[k] for k in config_keys} # will be useful for logging\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if distributed:\n",
    "    local_rank, device, num_devices = set_ddp()\n",
    "autoenc\n",
    "# .to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_cont:\n",
    "    mixup_pct = -1\n",
    "    if cont_model == 'cnx':\n",
    "        cnx = ConvnextXL('../train_logs/models/convnext_xlarge_alpha0.75_fullckpt.pth')\n",
    "        cnx.requires_grad_(False)\n",
    "        cnx.eval()\n",
    "        cnx\n",
    "        #.to(device)\n",
    "    train_augs = AugmentationSequential(\n",
    "        # kornia.augmentation.RandomCrop((480, 480), p=0.3),\n",
    "        # kornia.augmentation.Resize((512, 512)),\n",
    "        kornia.augmentation.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1, p=0.8),\n",
    "        kornia.augmentation.RandomGrayscale(p=0.2),\n",
    "        kornia.augmentation.RandomSolarize(p=0.2),\n",
    "        kornia.augmentation.RandomGaussianBlur(kernel_size=(7, 7), sigma=(0.1, 2.0), p=0.1),\n",
    "        kornia.augmentation.RandomResizedCrop((512, 512), scale=(0.5, 1.0)),\n",
    "        data_keys=[\"input\"],\n",
    "    )\n",
    "\n",
    "outdir = f'../train_logs/models/{model_name}/'\n",
    "if local_rank==0:\n",
    "    os.makedirs(outdir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> auto resume </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(os.path.join(outdir, 'last.pth')):\n",
    "    ckpt_path = os.path.join(outdir, 'last.pth')\n",
    "    resume_from_ckpt = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if num_devices==0: num_devices = 1\n",
    "num_workers = num_devices\n",
    "\n",
    "cache_dir = 'cache'\n",
    "n_cache_recs = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Prep models and data loaders </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating voxel2sd...\n"
     ]
    }
   ],
   "source": [
    "if local_rank == 0: print('Creating voxel2sd...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dims = {'01': 15724, '02': 14278, '05': 13039, '07':12682}\n",
    "if voxel_dims == 1: # 1D data\n",
    "    voxel2sd = Voxel2StableDiffusionModel(use_cont=use_cont, in_dim=in_dims[subj_id], ups_mode=ups_mode)\n",
    "elif voxel_dims == 3: # 3D data\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param counts:\n",
      "206,065,604 total\n",
      "206,065,604 trainable\n"
     ]
    }
   ],
   "source": [
    "voxel2sd\n",
    "#.to(device)\n",
    "voxel2sd = torch.nn.SyncBatchNorm.convert_sync_batchnorm(voxel2sd)\n",
    "if distributed:\n",
    "    voxel2sd = DDP(voxel2sd)\n",
    "\n",
    "try:\n",
    "    utils.count_params(voxel2sd)\n",
    "except:\n",
    "    if local_rank == 0: print('Cannot count params for voxel2sd (probably because it has Lazy layers)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pulling NSD webdataset data...\n"
     ]
    }
   ],
   "source": [
    "if local_rank == 0: print('Pulling NSD webdataset data...')\n",
    "\n",
    "# train_url = f\"{{/fsx/proj-fmri/shared/natural-scenes-dataset/webdataset_avg_split/train/train_subj{subj_id}_{{0..17}}.tar,/fsx/proj-fmri/shared/natural-scenes-dataset/webdataset_avg_split/val/val_subj{subj_id}_0.tar}}\"\n",
    "# val_url = f\"/fsx/proj-fmri/shared/natural-scenes-dataset/webdataset_avg_split/test/test_subj{subj_id}_{{0..1}}.tar\"\n",
    "# meta_url = f\"/fsx/proj-fmri/shared/natural-scenes-dataset/webdataset_avg_split/metadata_subj{subj_id}.json\"\n",
    "\n",
    "train_url = f\"../data/train_subj01_\"+\"{0..2}.tar\"\n",
    "val_url = f\"../data/val_subj01_0.tar\"\n",
    "meta_url = f\"../data/metadata_subj01.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepping train and validation dataloaders...\n"
     ]
    }
   ],
   "source": [
    "if local_rank == 0: print('Prepping train and validation dataloaders...')\n",
    "num_train = 8559 + 300\n",
    "num_val = 982"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting dataloaders...\n",
      "\n",
      "num_train 8859\n",
      "global_batch_size 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size 8\n",
      "num_workers 1\n",
      "num_batches 1107\n",
      "num_worker_batches 1107\n",
      "\n",
      "num_val 982\n",
      "val_num_batches 122\n",
      "val_batch_size 16\n"
     ]
    }
   ],
   "source": [
    "train_dl, val_dl, num_train, num_val = utils.get_dataloaders(\n",
    "    batch_size,\n",
    "    num_devices=num_devices,\n",
    "    num_workers=num_workers,\n",
    "    train_url=train_url,\n",
    "    val_url=val_url,\n",
    "    meta_url=meta_url,\n",
    "    val_batch_size=max(16, batch_size),\n",
    "    cache_dir='/tmp/wds-cache',\n",
    "    seed=seed+local_rank,\n",
    "    voxels_key='nsdgeneral.npy',\n",
    "    local_rank=local_rank,\n",
    "    num_train=num_train,\n",
    "    num_val=num_val\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "opt_grouped_parameters = [\n",
    "    {'params': [p for n, p in voxel2sd.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 1e-2},\n",
    "    {'params': [p for n, p in voxel2sd.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(opt_grouped_parameters, lr=1e-3)\n",
    "lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=max_lr, \n",
    "                                            total_steps=num_epochs*((num_train//batch_size)//num_devices), \n",
    "                                            final_div_factor=1000,\n",
    "                                            last_epoch=-1, pct_start=2/num_epochs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_ckpt(tag):\n",
    "    ckpt_path = os.path.join(outdir, f'{tag}.pth')\n",
    "    if tag == \"last\":\n",
    "        if os.path.exists(ckpt_path):\n",
    "            shutil.copyfile(ckpt_path, os.path.join(outdir, f'{tag}_old.pth'))\n",
    "    print(f'saving {ckpt_path}')\n",
    "    if local_rank==0:\n",
    "        state_dict = voxel2sd.state_dict()\n",
    "        for key in list(state_dict.keys()):\n",
    "            if 'module.' in key:\n",
    "                state_dict[key.replace('module.', '')] = state_dict[key]\n",
    "                del state_dict[key]\n",
    "        try:\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': state_dict,\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_losses': losses,\n",
    "                'val_losses': val_losses,\n",
    "                'lrs': lrs,\n",
    "                }, ckpt_path)\n",
    "        except:\n",
    "            print('Failed to save weights')\n",
    "            print(traceback.format_exc())\n",
    "    if tag == \"last\":\n",
    "        if os.path.exists(os.path.join(outdir, f'{tag}_old.pth')):\n",
    "            os.remove(os.path.join(outdir, f'{tag}_old.pth'))\n",
    "\n",
    "        # if wandb_log:\n",
    "        #     wandb.save(ckpt_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Optionally resume from checkpoint </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done with model preparations!\n"
     ]
    }
   ],
   "source": [
    "if resume_from_ckpt:\n",
    "    print(\"\\n---resuming from ckpt_path---\\n\", ckpt_path)\n",
    "    checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "    epoch = checkpoint['epoch']+1\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict']) \n",
    "    if hasattr(voxel2sd, 'module'):\n",
    "        voxel2sd.module.load_state_dict(checkpoint['model_state_dict'])\n",
    "    else:\n",
    "        voxel2sd.load_state_dict(checkpoint['model_state_dict'])\n",
    "    total_steps_done = epoch*((num_train//batch_size)//num_devices)\n",
    "    for _ in range(total_steps_done):\n",
    "        lr_scheduler.step()\n",
    "    del checkpoint\n",
    "    torch.cuda.empty_cache()\n",
    "else:\n",
    "    epoch = 0\n",
    "\n",
    "if local_rank==0: print(\"\\nDone with model preparations!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Model? </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                         | 0/120 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "progress_bar = tqdm(range(epoch, num_epochs), ncols=150, disable=(local_rank!=0))\n",
    "losses = []\n",
    "val_losses = []\n",
    "lrs = []\n",
    "best_val_loss = 1e10\n",
    "best_ssim = 0\n",
    "# mean = torch.tensor([0.485, 0.456, 0.406]).to(device).reshape(1,3,1,1)\n",
    "# std = torch.tensor([0.228, 0.224, 0.225]).to(device).reshape(1,3,1,1)\n",
    "mean = torch.tensor([0.485, 0.456, 0.406]).reshape(1,3,1,1)\n",
    "std = torch.tensor([0.228, 0.224, 0.225]).reshape(1,3,1,1)\n",
    "epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ckpt_path is not None:\n",
    "    print(\"\\n---resuming from ckpt_path---\\n\",ckpt_path)\n",
    "    checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "    epoch = checkpoint['epoch']+1\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])        \n",
    "    voxel2sd.module.load_state_dict(checkpoint['model_state_dict'])\n",
    "    global_batch_size = batch_size * num_devices\n",
    "    total_steps_done = epoch*(num_train//global_batch_size)\n",
    "    for _ in range(total_steps_done):\n",
    "        lr_scheduler.step()\n",
    "    del checkpoint\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                                                         | 0/120 [00:05<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "progress_bar = tqdm(range(epoch, num_epochs), ncols=150, disable=(local_rank!=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lenovo\\miniconda3\\envs\\mindeye\\lib\\site-packages\\controlnet_aux\\mediapipe_face\\mediapipe_face_common.py:7: UserWarning: The module 'mediapipe' is not installed. The package will have limited functionality. Please install it using the command: pip install 'mediapipe'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from controlnet_aux.midas import MidasDetector \n",
    "midas_depth = MidasDetector.from_pretrained(\n",
    "  \"valhalla/t2iadapter-aux-models\", filename=\"dpt_large_384.pt\", model_type=\"dpt_large\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> torch.Size([256, 256, 3])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at ..\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 30485160000 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\lenovo\\Desktop\\fMRI-reconstruction-NSD\\src\\train_autoencoder.ipynb Cell 33\u001b[0m line \u001b[0;36m7\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/lenovo/Desktop/fMRI-reconstruction-NSD/src/train_autoencoder.ipynb#X43sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# image_512 = F.interpolate(image, (512, 512), mode='bilinear', align_corners=False, antialias=True)\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/lenovo/Desktop/fMRI-reconstruction-NSD/src/train_autoencoder.ipynb#X43sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# print(type(image_512), image_512.shape)\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/lenovo/Desktop/fMRI-reconstruction-NSD/src/train_autoencoder.ipynb#X43sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m test \u001b[39m=\u001b[39m transforms\u001b[39m.\u001b[39mToPILImage()(image[\u001b[39m0\u001b[39m])\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/lenovo/Desktop/fMRI-reconstruction-NSD/src/train_autoencoder.ipynb#X43sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m image_depth \u001b[39m=\u001b[39m midas_depth(test, detect_resolution\u001b[39m=\u001b[39m\u001b[39m256\u001b[39m, image_resolution\u001b[39m=\u001b[39m\u001b[39m512\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/lenovo/Desktop/fMRI-reconstruction-NSD/src/train_autoencoder.ipynb#X43sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mtype\u001b[39m(image_depth))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/lenovo/Desktop/fMRI-reconstruction-NSD/src/train_autoencoder.ipynb#X43sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# print(train_i)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lenovo/Desktop/fMRI-reconstruction-NSD/src/train_autoencoder.ipynb#X43sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m# print(type(image_depth), image_depth.size)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\lenovo\\miniconda3\\envs\\mindeye\\lib\\site-packages\\controlnet_aux\\midas\\__init__.py:57\u001b[0m, in \u001b[0;36mMidasDetector.__call__\u001b[1;34m(self, input_image, a, bg_th, depth_and_normal, detect_resolution, image_resolution, output_type)\u001b[0m\n\u001b[0;32m     55\u001b[0m image_depth \u001b[39m=\u001b[39m image_depth \u001b[39m/\u001b[39m \u001b[39m127.5\u001b[39m \u001b[39m-\u001b[39m \u001b[39m1.0\u001b[39m\n\u001b[0;32m     56\u001b[0m image_depth \u001b[39m=\u001b[39m rearrange(image_depth, \u001b[39m'\u001b[39m\u001b[39mh w c -> 1 c h w\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> 57\u001b[0m depth \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(image_depth)[\u001b[39m0\u001b[39m]\n\u001b[0;32m     59\u001b[0m depth_pt \u001b[39m=\u001b[39m depth\u001b[39m.\u001b[39mclone()\n\u001b[0;32m     60\u001b[0m depth_pt \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmin(depth_pt)\n",
      "File \u001b[1;32mc:\\Users\\lenovo\\miniconda3\\envs\\mindeye\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\lenovo\\miniconda3\\envs\\mindeye\\lib\\site-packages\\controlnet_aux\\midas\\api.py:167\u001b[0m, in \u001b[0;36mMiDaSInference.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    165\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m    166\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m--> 167\u001b[0m         prediction \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(x)\n\u001b[0;32m    168\u001b[0m     \u001b[39mreturn\u001b[39;00m prediction\n",
      "File \u001b[1;32mc:\\Users\\lenovo\\miniconda3\\envs\\mindeye\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\lenovo\\miniconda3\\envs\\mindeye\\lib\\site-packages\\controlnet_aux\\midas\\midas\\dpt_depth.py:108\u001b[0m, in \u001b[0;36mDPTDepthModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m--> 108\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mforward(x)\u001b[39m.\u001b[39msqueeze(dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\lenovo\\miniconda3\\envs\\mindeye\\lib\\site-packages\\controlnet_aux\\midas\\midas\\dpt_depth.py:71\u001b[0m, in \u001b[0;36mDPT.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchannels_last \u001b[39m==\u001b[39m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m     x\u001b[39m.\u001b[39mcontiguous(memory_format\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mchannels_last)\n\u001b[1;32m---> 71\u001b[0m layer_1, layer_2, layer_3, layer_4 \u001b[39m=\u001b[39m forward_vit(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpretrained, x)\n\u001b[0;32m     73\u001b[0m layer_1_rn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscratch\u001b[39m.\u001b[39mlayer1_rn(layer_1)\n\u001b[0;32m     74\u001b[0m layer_2_rn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscratch\u001b[39m.\u001b[39mlayer2_rn(layer_2)\n",
      "File \u001b[1;32mc:\\Users\\lenovo\\miniconda3\\envs\\mindeye\\lib\\site-packages\\controlnet_aux\\midas\\midas\\vit.py:59\u001b[0m, in \u001b[0;36mforward_vit\u001b[1;34m(pretrained, x)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward_vit\u001b[39m(pretrained, x):\n\u001b[0;32m     57\u001b[0m     b, c, h, w \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mshape\n\u001b[1;32m---> 59\u001b[0m     glob \u001b[39m=\u001b[39m pretrained\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mforward_flex(x)\n\u001b[0;32m     61\u001b[0m     layer_1 \u001b[39m=\u001b[39m pretrained\u001b[39m.\u001b[39mactivations[\u001b[39m\"\u001b[39m\u001b[39m1\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m     62\u001b[0m     layer_2 \u001b[39m=\u001b[39m pretrained\u001b[39m.\u001b[39mactivations[\u001b[39m\"\u001b[39m\u001b[39m2\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\lenovo\\miniconda3\\envs\\mindeye\\lib\\site-packages\\controlnet_aux\\midas\\midas\\vit.py:149\u001b[0m, in \u001b[0;36mforward_flex\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    146\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpos_drop(x)\n\u001b[0;32m    148\u001b[0m \u001b[39mfor\u001b[39;00m blk \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblocks:\n\u001b[1;32m--> 149\u001b[0m     x \u001b[39m=\u001b[39m blk(x)\n\u001b[0;32m    151\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm(x)\n\u001b[0;32m    153\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\lenovo\\miniconda3\\envs\\mindeye\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\lenovo\\miniconda3\\envs\\mindeye\\lib\\site-packages\\timm\\models\\vision_transformer.py:155\u001b[0m, in \u001b[0;36mBlock.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m--> 155\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdrop_path1(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mls1(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattn(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm1(x))))\n\u001b[0;32m    156\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdrop_path2(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mls2(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmlp(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm2(x))))\n\u001b[0;32m    157\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\lenovo\\miniconda3\\envs\\mindeye\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\lenovo\\miniconda3\\envs\\mindeye\\lib\\site-packages\\timm\\models\\vision_transformer.py:92\u001b[0m, in \u001b[0;36mAttention.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     91\u001b[0m     q \u001b[39m=\u001b[39m q \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscale\n\u001b[1;32m---> 92\u001b[0m     attn \u001b[39m=\u001b[39m q \u001b[39m@\u001b[39;49m k\u001b[39m.\u001b[39;49mtranspose(\u001b[39m-\u001b[39;49m\u001b[39m2\u001b[39;49m, \u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[0;32m     93\u001b[0m     attn \u001b[39m=\u001b[39m attn\u001b[39m.\u001b[39msoftmax(dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     94\u001b[0m     attn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattn_drop(attn)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at ..\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 30485160000 bytes."
     ]
    }
   ],
   "source": [
    "for train_i, (voxel, image, _) in enumerate(val_dl):\n",
    "    print(type(image), image[0].reshape((256, 256, 3)).shape)\n",
    "    image = image[0].reshape((256, 256, 3))\n",
    "    # image_512 = F.interpolate(image, (512, 512), mode='bilinear', align_corners=False, antialias=True)\n",
    "    # print(type(image_512), image_512.shape)\n",
    "    test = transforms.ToPILImage()(image[0])\n",
    "    image_depth = midas_depth(test, detect_resolution=256, image_resolution=512)\n",
    "    print(type(image_depth))\n",
    "    # print(train_i)\n",
    "    # print(type(image_depth), image_depth.size)\n",
    "    break\n",
    "print(image)\n",
    "\n",
    "Image.fromarray(image_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQAAAAEACAIAAADTED8xAAAD5UlEQVR4Ae3ayQqEMAwA0Nb//+cZqgcvIoI1NO3zIOLSJC9uiKWYCBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAYVaCOmpi8CBAgQIDAS4GbZ1y92fYyaqbDV1dQf+jZuoVGE4zAM4GFnwbnHfBceoY2x15rVj1H71RBgAABAgQSCXjlSNQsqX4t4HL4Wni08XV8tI7IhwABAgQIECBAgAABAgQIECBAgAABAtcC7Xtu3afr7dYSIEBgWoH2/2i7/x2zactUGAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAhECtTIYKlikdnbtTxDLcsTpLpueycb3P2td/4dxvt1GCPvEMEnQF4omRMgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBA4BP4HxwErm5HjawAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=256x256>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image.fromarray(image.numpy().astype(np.uint8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\lenovo\\Desktop\\fMRI-reconstruction-NSD\\src\\train_autoencoder.ipynb Cell 33\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lenovo/Desktop/fMRI-reconstruction-NSD/src/train_autoencoder.ipynb#X44sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m image \u001b[39m=\u001b[39m image\u001b[39m.\u001b[39mfloat()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lenovo/Desktop/fMRI-reconstruction-NSD/src/train_autoencoder.ipynb#X44sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m image_512 \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39minterpolate(image, (\u001b[39m512\u001b[39m, \u001b[39m512\u001b[39m), mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbilinear\u001b[39m\u001b[39m'\u001b[39m, align_corners\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, antialias\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/lenovo/Desktop/fMRI-reconstruction-NSD/src/train_autoencoder.ipynb#X44sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m image_depth \u001b[39m=\u001b[39m midas_depth(image, detect_resolution\u001b[39m=\u001b[39m\u001b[39m512\u001b[39m, image_resolution\u001b[39m=\u001b[39m\u001b[39m512\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lenovo/Desktop/fMRI-reconstruction-NSD/src/train_autoencoder.ipynb#X44sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m voxel \u001b[39m=\u001b[39m voxel\u001b[39m.\u001b[39mto(device)\u001b[39m.\u001b[39mfloat()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lenovo/Desktop/fMRI-reconstruction-NSD/src/train_autoencoder.ipynb#X44sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m voxel \u001b[39m=\u001b[39m utils\u001b[39m.\u001b[39mvoxel_select(voxel)\n",
      "File \u001b[1;32mc:\\Users\\lenovo\\miniconda3\\envs\\mindeye\\lib\\site-packages\\controlnet_aux\\midas\\__init__.py:47\u001b[0m, in \u001b[0;36mMidasDetector.__call__\u001b[1;34m(self, input_image, a, bg_th, depth_and_normal, detect_resolution, image_resolution, output_type)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     45\u001b[0m     output_type \u001b[39m=\u001b[39m output_type \u001b[39mor\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnp\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m---> 47\u001b[0m input_image \u001b[39m=\u001b[39m HWC3(input_image)\n\u001b[0;32m     48\u001b[0m input_image \u001b[39m=\u001b[39m resize_image(input_image, detect_resolution)\n\u001b[0;32m     50\u001b[0m \u001b[39massert\u001b[39;00m input_image\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m3\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\lenovo\\miniconda3\\envs\\mindeye\\lib\\site-packages\\controlnet_aux\\util.py:15\u001b[0m, in \u001b[0;36mHWC3\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mif\u001b[39;00m x\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[0;32m     14\u001b[0m     x \u001b[39m=\u001b[39m x[:, :, \u001b[39mNone\u001b[39;00m]\n\u001b[1;32m---> 15\u001b[0m \u001b[39massert\u001b[39;00m x\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m3\u001b[39m\n\u001b[0;32m     16\u001b[0m H, W, C \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mshape\n\u001b[0;32m     17\u001b[0m \u001b[39massert\u001b[39;00m C \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mor\u001b[39;00m C \u001b[39m==\u001b[39m \u001b[39m3\u001b[39m \u001b[39mor\u001b[39;00m C \u001b[39m==\u001b[39m \u001b[39m4\u001b[39m\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in progress_bar:\n",
    "    voxel2sd.train()\n",
    "    \n",
    "    loss_mse_sum = 0\n",
    "    loss_reconst_sum = 0\n",
    "    loss_cont_sum = 0\n",
    "    loss_sobel_sum = 0\n",
    "    val_loss_mse_sum = 0\n",
    "    val_loss_reconst_sum = 0\n",
    "    val_ssim_score_sum = 0\n",
    "\n",
    "    reconst_fails = []\n",
    "\n",
    "    for train_i, (voxel, image, _) in enumerate(train_dl):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # image = image.to(device).float()\n",
    "        image = image.float()\n",
    "        image_512 = F.interpolate(image, (512, 512), mode='bilinear', align_corners=False, antialias=True)\n",
    "        image_depth = midas_depth(image, detect_resolution=512, image_resolution=512)\n",
    "        voxel = voxel.to(device).float()\n",
    "        voxel = utils.voxel_select(voxel)\n",
    "        if epoch <= mixup_pct * num_epochs:\n",
    "            voxel, perm, betas, select = utils.mixco(voxel)\n",
    "        else:\n",
    "            select = None\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=use_mp):\n",
    "            autoenc_image = kornia.filters.median_blur(image_depth, (15, 15)) if use_blurred_training else image_depth\n",
    "            image_enc = autoenc.encode(2*autoenc_image-1).latent_dist.mode() * 0.18215\n",
    "            if use_cont:\n",
    "                image_enc_pred, transformer_feats = voxel2sd(voxel, return_transformer_feats=True)\n",
    "            else:\n",
    "                image_enc_pred = voxel2sd(voxel)\n",
    "            \n",
    "            if epoch <= mixup_pct * num_epochs:\n",
    "                image_enc_shuf = image_enc[perm]\n",
    "                betas_shape = [-1] + [1]*(len(image_enc.shape)-1)\n",
    "                image_enc[select] = image_enc[select] * betas[select].reshape(*betas_shape) + \\\n",
    "                    image_enc_shuf[select] * (1 - betas[select]).reshape(*betas_shape)\n",
    "            \n",
    "            if use_cont:\n",
    "                image_norm = (image_512 - mean)/std\n",
    "                image_aug = (train_augs(image_512) - mean)/std\n",
    "                _, cnx_embeds = cnx(image_norm)\n",
    "                _, cnx_aug_embeds = cnx(image_aug)\n",
    "\n",
    "                cont_loss = utils.soft_cont_loss(\n",
    "                    F.normalize(transformer_feats.reshape(-1, transformer_feats.shape[-1]), dim=-1),\n",
    "                    F.normalize(cnx_embeds.reshape(-1, cnx_embeds.shape[-1]), dim=-1),\n",
    "                    F.normalize(cnx_aug_embeds.reshape(-1, cnx_embeds.shape[-1]), dim=-1),\n",
    "                    temp=0.075,\n",
    "                    distributed=distributed\n",
    "                )\n",
    "                del image_aug, cnx_embeds, transformer_feats\n",
    "            else:\n",
    "                cont_loss = torch.tensor(0)\n",
    "\n",
    "            # mse_loss = F.mse_loss(image_enc_pred, image_enc)/0.18215\n",
    "            mse_loss = F.l1_loss(image_enc_pred, image_enc)\n",
    "            del image_depth, voxel\n",
    "\n",
    "            if use_reconst: #epoch >= 0.1 * num_epochs:\n",
    "                # decode only non-mixed images\n",
    "                if select is not None:\n",
    "                    selected_inds = torch.where(~select)[0]\n",
    "                    reconst_select = selected_inds[torch.randperm(len(selected_inds))][:4] \n",
    "                else:\n",
    "                    reconst_select = torch.arange(len(image_enc_pred))\n",
    "                image_enc_pred = F.interpolate(image_enc_pred[reconst_select], scale_factor=0.5, mode='bilinear', align_corners=False)\n",
    "                reconst = autoenc.decode(image_enc_pred/0.18215).sample\n",
    "                # reconst_loss = F.mse_loss(reconst, 2*image[reconst_select]-1)\n",
    "                reconst_image = kornia.filters.median_blur(image[reconst_select], (7, 7)) if use_blurred_training else image[reconst_select]\n",
    "                reconst_loss = F.l1_loss(reconst, 2*reconst_image-1)\n",
    "                if reconst_loss != reconst_loss:\n",
    "                    reconst_loss = torch.tensor(0)\n",
    "                    reconst_fails.append(train_i) \n",
    "                if use_sobel_loss:\n",
    "                    sobel_targ = kornia.filters.sobel(kornia.filters.median_blur(image[reconst_select], (3,3)))\n",
    "                    sobel_pred = kornia.filters.sobel(reconst/2 + 0.5)\n",
    "                    sobel_loss = F.l1_loss(sobel_pred, sobel_targ)\n",
    "                else:\n",
    "                    sobel_loss = torch.tensor(0)\n",
    "            else:\n",
    "                reconst_loss = torch.tensor(0)\n",
    "                sobel_loss = torch.tensor(0)\n",
    "            \n",
    "\n",
    "            loss = mse_loss/0.18215 + 2*reconst_loss + 0.1*cont_loss + 16*sobel_loss\n",
    "            # utils.check_loss(loss)\n",
    "\n",
    "            loss_mse_sum += mse_loss.item()\n",
    "            loss_reconst_sum += reconst_loss.item()\n",
    "            loss_cont_sum += cont_loss.item()\n",
    "            loss_sobel_sum += sobel_loss.item()\n",
    "\n",
    "            losses.append(loss.item())\n",
    "            lrs.append(optimizer.param_groups[0]['lr'])\n",
    "\n",
    "            if local_rank==0:\n",
    "                logs = OrderedDict(\n",
    "                    train_loss=np.mean(losses[-(train_i+1):]),\n",
    "                    val_loss=np.nan,\n",
    "                    lr=lrs[-1],\n",
    "                )\n",
    "                progress_bar.set_postfix(**logs)\n",
    "        \n",
    "        loss.backward()\n",
    "        # if reconst_loss > 0:\n",
    "        #     torch.nn.utils.clip_grad_norm_(voxel2sd.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        if lr_scheduler is not None:\n",
    "            lr_scheduler.step()\n",
    "\n",
    "    if local_rank==0: \n",
    "        voxel2sd.eval()\n",
    "        for val_i, (voxel, image, _) in enumerate(val_dl): \n",
    "            with torch.inference_mode():\n",
    "                image = image.to(device).float()\n",
    "                #image = F.interpolate(image, (512, 512), mode='bilinear', align_corners=False, antialias=True)       \n",
    "                image = midas_depth(image, detect_resolution=512, image_resolution=512)       \n",
    "                voxel = voxel.to(device).float()\n",
    "                voxel = voxel.mean(1)\n",
    "                \n",
    "                with torch.cuda.amp.autocast(enabled=use_mp):\n",
    "                    image_enc = autoenc.encode(2*image-1).latent_dist.mode() * 0.18215\n",
    "                    if hasattr(voxel2sd, 'module'):\n",
    "                        image_enc_pred = voxel2sd.module(voxel)\n",
    "                    else:\n",
    "                        image_enc_pred = voxel2sd(voxel)\n",
    "\n",
    "                    mse_loss = F.mse_loss(image_enc_pred, image_enc)\n",
    "                    \n",
    "                    if use_reconst:\n",
    "                        reconst = autoenc.decode(image_enc_pred[-16:]/0.18215).sample\n",
    "                        image = image[-16:]\n",
    "                        reconst_loss = F.mse_loss(reconst, 2*image-1)\n",
    "                        ssim_score = ssim((reconst/2 + 0.5).clamp(0,1), image, data_range=1, size_average=True, nonnegative_ssim=True)\n",
    "                    else:\n",
    "                        reconst = None\n",
    "                        reconst_loss = torch.tensor(0)\n",
    "                        ssim_score = torch.tensor(0)\n",
    "\n",
    "                    val_loss_mse_sum += mse_loss.item()\n",
    "                    val_loss_reconst_sum += reconst_loss.item()\n",
    "                    val_ssim_score_sum += ssim_score.item()\n",
    "\n",
    "                    val_losses.append(mse_loss.item() + reconst_loss.item())        \n",
    "\n",
    "            logs = OrderedDict(\n",
    "                train_loss=np.mean(losses[-(train_i+1):]),\n",
    "                val_loss=np.mean(val_losses[-(val_i+1):]),\n",
    "                lr=lrs[-1],\n",
    "            )\n",
    "            progress_bar.set_postfix(**logs)\n",
    "\n",
    "        if (not save_at_end and ckpt_saving) or (save_at_end and epoch == num_epochs - 1):\n",
    "            # save best model\n",
    "            val_loss = np.mean(val_losses[-(val_i+1):])\n",
    "            val_ssim = val_ssim_score_sum / (val_i + 1)\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                save_ckpt('best')\n",
    "            else:\n",
    "                print(f'not best - val_loss: {val_loss:.3f}, best_val_loss: {best_val_loss:.3f}')\n",
    "            if val_ssim > best_ssim:\n",
    "                best_ssim = val_ssim\n",
    "                save_ckpt('best_ssim')\n",
    "            else:\n",
    "                print(f'not best - val_ssim: {val_ssim:.3f}, best_ssim: {best_ssim:.3f}')\n",
    "\n",
    "            save_ckpt('last')\n",
    "            # Save model checkpoint every `ckpt_interval`` epochs or on the last epoch\n",
    "            if (ckpt_interval is not None and (epoch + 1) % ckpt_interval == 0) or epoch == num_epochs - 1:\n",
    "                save_ckpt(f'epoch{(epoch+1):03d}')\n",
    "            try:\n",
    "                orig = image\n",
    "                if reconst is None:\n",
    "                    reconst = autoenc.decode(image_enc_pred[-16:].detach()/0.18215).sample\n",
    "                    orig = image[-16:]\n",
    "                pred_grid = make_grid(((reconst/2 + 0.5).clamp(0,1)*255).byte(), nrow=int(len(reconst)**0.5)).permute(1,2,0).cpu().numpy()\n",
    "                orig_grid = make_grid((orig*255).byte(), nrow=int(len(orig)**0.5)).permute(1,2,0).cpu().numpy()\n",
    "                comb_grid = np.concatenate([orig_grid, pred_grid], axis=1)\n",
    "                del pred_grid, orig_grid\n",
    "                Image.fromarray(comb_grid).save(f'{outdir}/reconst_epoch{(epoch+1):03d}.png')\n",
    "            except:\n",
    "                print(\"Failed to save reconst image\")\n",
    "                print(traceback.format_exc())\n",
    "\n",
    "        logs = {\n",
    "            \"train/loss\": np.mean(losses[-(train_i+1):]),\n",
    "            \"val/loss\": np.mean(val_losses[-(val_i+1):]),\n",
    "            \"train/lr\": lrs[-1],\n",
    "            \"train/num_steps\": len(losses),\n",
    "            \"train/loss_mse\": loss_mse_sum / (train_i + 1),\n",
    "            \"train/loss_reconst\": loss_reconst_sum / (train_i + 1),\n",
    "            \"train/loss_cont\": loss_cont_sum / (train_i + 1),\n",
    "            \"train/loss_sobel\": loss_sobel_sum / (train_i + 1),\n",
    "            \"val/loss_mse\": val_loss_mse_sum / (val_i + 1),\n",
    "            \"val/loss_reconst\": val_loss_reconst_sum / (val_i + 1),\n",
    "            \"val/ssim\": val_ssim_score_sum / (val_i + 1),\n",
    "        }\n",
    "        if local_rank==0: print(logs)\n",
    "        if len(reconst_fails) > 0 and local_rank==0:\n",
    "            print(f'Reconst fails {len(reconst_fails)}/{train_i}: {reconst_fails}')\n",
    "\n",
    "    if distributed:\n",
    "        dist.barrier()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mindeye",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
